name: Big Data Pipeline Integration Test

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  setup-data:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v4
        - uses: astral-sh/setup-uv@v1
        - name: Generate Dummy Test Data
          run: |
            mkdir -p data
            uv venv .venv
            uv pip install pandas numpy pyarrow
            uv run python -c "
            import pandas as pd
            import numpy as np
            import pyarrow as pa
            import pyarrow.parquet as pq
            
            num_periods = 3 * 24 * 60 
            
            df = pd.DataFrame({
                'VendorID': [1, 2] * (num_periods // 2),
                'tpep_pickup_datetime': pd.date_range(start='2025-08-01', periods=num_periods, freq='min'),
                'tpep_dropoff_datetime': pd.date_range(start='2025-08-01 00:10:00', periods=num_periods, freq='min'),
                'passenger_count': np.random.randint(1, 5, num_periods),
                'trip_distance': np.random.rand(num_periods) * 10,
                'PULocationID': np.random.randint(1, 263, num_periods),
                'DOLocationID': np.random.randint(1, 263, num_periods),
                'total_amount': np.random.rand(num_periods) * 50
            })
            table = pa.Table.from_pandas(df)
            pq.write_table(table, 'data/yellow_tripdata_2025-08.parquet')
            "
        - name: Upload Data
          uses: actions/upload-artifact@v4
          with:
            name: test-data
            path: data/yellow_tripdata_2025-08.parquet

  test-stream:
    needs: setup-data
    name: Stream Processing Test
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Download Data
      uses: actions/download-artifact@v4
      with:
        name: test-data
        path: data

    - name: Start Stream Services
      run: docker compose up -d broker cassandra init-cassandra spark producer

    - name: Run Stream Verification
      run: uv run --with cassandra-driver tests/integration_test.py --table zone_performance_30m

    - name: Dump Logs on Failure
      if: failure()
      run: docker compose logs

  test-batch:
    needs: setup-data
    name: Batch Processing Test
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Download Data
      uses: actions/download-artifact@v4
      with:
        name: test-data
        path: data

    - name: Start Batch Services
      run: docker compose up -d broker cassandra init-cassandra spark-batch producer

    - name: Run Batch Verification
      run: uv run --with cassandra-driver tests/integration_test.py --table zone_performance_1d

    - name: Dump Logs on Failure
      if: failure()
      run: docker compose logs
