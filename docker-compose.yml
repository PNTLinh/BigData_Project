version: '3.8'

services:
  broker:
    image: apache/kafka:latest
    user: root
    hostname: broker
    container_name: broker
    ports:
      - "9092:9092"
    volumes:
      - kafka_data:/tmp/kraft-combined-logs
    environment:
      KAFKA_HEAP_OPTS: "-Xmx400M -Xms400M"
      KAFKA_BROKER_ID: "1"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: "1"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@broker:29093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      # Sửa: Đảm bảo các listener trỏ đúng hostname
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,PLAINTEXT_HOST://0.0.0.0:9092"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
      CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"
    deploy:
      resources:
        limits:
          memory: 600M

  producer: 
    build: 
      context: .
      dockerfile: kafka/Dockerfile
    container_name: mta-ace-stream
    depends_on:
      - broker
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
    # Sửa: Dùng wait-for-it hoặc sleep lâu hơn để Kafka kịp Ready
    command: sh -c "sleep 30 && python /app/kafka/producer.py" 
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M

  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    ports:
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=TaxiCluster
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_RACK=rack1
      - MAX_HEAP_SIZE=512M
      - HEAP_NEWSIZE=128M

    volumes:
      - cassandra_data:/var/lib/cassandra
      - ./cassandra:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -e 'describe keyspaces'"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 2G

  spark:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark
    depends_on:
      cassandra:
        condition: service_healthy
      broker:
        condition: service_started
      namenode:
        condition: service_started
    ports:
      - "4040:4040"
    volumes:
      - ./spark:/opt/spark-apps
      - spark-checkpoints:/tmp/spark_checkpoints
      - spark-ivy:/tmp/.ivy2
    user: "0:0"
    environment:
      - HOME=/root
      - USER=root
      - LOGNAME=root
      - HADOOP_USER_NAME=root
      - SPARK_USER=root

      - SPARK_LOCAL_DIRS=/tmp

      # App config
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
      - KAFKA_TOPIC=mta-ace-stream
      - CASSANDRA_HOST=cassandra
      - CASSANDRA_KEYSPACE=mta_data
      - CASSANDRA_TABLE=train_updates
      - HDFS_PATH=hdfs://namenode:8020/mta/history
      - CHECKPOINT_ROOT=/tmp/spark_checkpoints

    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 1g
      --executor-memory 1g
      --conf spark.jars.ivy=/tmp/.ivy2
      --conf spark.hadoop.hadoop.security.authentication=simple
      --conf spark.hadoop.hadoop.security.authorization=false
      --conf spark.driver.extraJavaOptions=-Duser.name=spark
      --conf spark.executor.extraJavaOptions=-Duser.name=spark
      --conf spark.sql.streaming.checkpointLocation=/tmp/spark_checkpoints
      /opt/spark-apps/streaming.py


    

    

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    environment:
      - CLUSTER_NAME=test
      - HDFS_CONF_dfs_namenode_rpc_address=namenode:8020
      - HDFS_CONF_dfs_namenode_http_address=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    deploy:
      resources:
        limits:
          memory: 800M

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_hostname=datanode
    depends_on:
      - namenode
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    deploy:
      resources:
        limits:
          memory: 800M

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=hadesarchitect-cassandra-datasource
    depends_on:
      cassandra:
        condition: service_healthy
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker:29092
    depends_on:
      - broker
    deploy:
      resources:
        limits:
          memory: 300M

volumes:
  cassandra_data:
  grafana_data:
  spark-checkpoints:
  spark-ivy:
  kafka_data:
  hdfs_namenode:
  hdfs_datanode: